{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running CapyMOA models with the paper's datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from capymoa.stream import stream_from_file\n",
    "from capymoa.anomaly import HalfSpaceTrees, OnlineIsolationForest, Autoencoder\n",
    "from capymoa.evaluation import AnomalyDetectionEvaluator\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, roc_curve, auc\n",
    "from scipy.stats import sem\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define datasets, their corresponding run counts, and models\n",
    "datasets = {\n",
    "    \"abalone\": 10,\n",
    "    \"annthyroid\": 10,\n",
    "    \"magicgamma\": 4,\n",
    "    \"kdd_ftp\": 10,\n",
    "    \"mammography\": 10,\n",
    "    \"thyroid\": 10,\n",
    "    \"mnist\": 10,\n",
    "    \"musk\": 5,\n",
    "    \"satellite\": 5,\n",
    "    \"satimages\": 5,\n",
    "    \"spambase\": 5,\n",
    "    \"shuttle_odds\": 2\n",
    "}\n",
    "\n",
    "models = {\n",
    "    \"HalfSpaceTrees\": HalfSpaceTrees,\n",
    "    \"Autoencoder\": Autoencoder,\n",
    "    \"OnlineIsolationForest\": OnlineIsolationForest\n",
    "}\n",
    "\n",
    "# Define dataset path\n",
    "dataset_path = r\"C:\\Users\\aleja\\OneDrive - Universidad Nacional de Colombia\\Documentos\\Institut Polytechnique de Paris\\courses\\P1\\Data Streaming\\project\\actual code\\datasets\\forStefan\\data\\public\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: abalone (Runs: 2)\n",
      "Running model: HalfSpaceTrees\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleja\\OneDrive - Universidad Nacional de Colombia\\Documentos\\Institut Polytechnique de Paris\\courses\\P1\\Data Streaming\\project\\actual code\\CapyMOA\\src\\capymoa\\stream\\_stream.py:38: UserWarning: target variable includes 2 (< 20) unique values, inferred as categorical, set target_type = 'numeric' if you intend numeric targets\n",
      "  warnings.warn(f'target variable includes {num_unique} (< 20) unique values, inferred as categorical, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1: AP = 0.5313, AUC = 0.9628, Time = 0.26s\n",
      "Run 2: AP = 0.5313, AUC = 0.9628, Time = 0.65s\n",
      "Checkpoint saved for model HalfSpaceTrees\n",
      "Summary for HalfSpaceTrees:\n",
      "AP: 0.5313 ± 0.0000 (95% CI)\n",
      "AUC (sklearn): 0.9628 ± 0.0000 (95% CI)\n",
      "AUC (paper): 0.9628 ± 0.0000 (95% CI)\n",
      "AUC (CapyMOA): 0.9628 ± 0.0000 (95% CI)\n",
      "Time: 0.45 ± 0.38 seconds (95% CI)\n",
      "Running model: Autoencoder\n",
      "Run 1: AP = 0.1163, AUC = 0.8546, Time = 1.65s\n",
      "Run 2: AP = 0.1163, AUC = 0.8546, Time = 1.39s\n",
      "Checkpoint saved for model Autoencoder\n",
      "Summary for Autoencoder:\n",
      "AP: 0.1163 ± 0.0000 (95% CI)\n",
      "AUC (sklearn): 0.8546 ± 0.0000 (95% CI)\n",
      "AUC (paper): 0.8546 ± 0.0000 (95% CI)\n",
      "AUC (CapyMOA): 0.8546 ± 0.0000 (95% CI)\n",
      "Time: 1.52 ± 0.25 seconds (95% CI)\n",
      "Running model: OnlineIsolationForest\n",
      "Run 1: AP = 0.0333, AUC = 0.3180, Time = 44.39s\n",
      "Run 2: AP = 0.0333, AUC = 0.3180, Time = 44.28s\n",
      "Checkpoint saved for model OnlineIsolationForest\n",
      "Summary for OnlineIsolationForest:\n",
      "AP: 0.0333 ± 0.0000 (95% CI)\n",
      "AUC (sklearn): 0.3180 ± 0.0000 (95% CI)\n",
      "AUC (paper): 0.3180 ± 0.0000 (95% CI)\n",
      "AUC (CapyMOA): 0.3180 ± 0.0000 (95% CI)\n",
      "Time: 44.33 ± 0.11 seconds (95% CI)\n"
     ]
    }
   ],
   "source": [
    "# Results storage\n",
    "all_results = []\n",
    "\n",
    "# Main loop\n",
    "for dataset_name, n_runs in datasets.items():\n",
    "    print(f\"Dataset: {dataset_name} (Runs: {n_runs})\")\n",
    "\n",
    "    input_path = os.path.join(dataset_path, f\"{dataset_name}.gz\")\n",
    "    output_path = os.path.join(dataset_path, f\"{dataset_name}.csv\")\n",
    "\n",
    "    # Unzip the dataset if needed\n",
    "    if not os.path.exists(output_path):\n",
    "        with gzip.open(input_path, 'rt') as gz_file:\n",
    "            df = pd.read_csv(gz_file)\n",
    "            df.to_csv(output_path, index=False)\n",
    "            print(f\"CSV saved to: {output_path}\")\n",
    "\n",
    "    stream = stream_from_file(output_path, dataset_name=dataset_name)\n",
    "    schema = stream.get_schema()\n",
    "\n",
    "    # Load labels for metrics\n",
    "    df = pd.read_csv(output_path)\n",
    "    labels = df['label'].to_numpy(dtype='float32')\n",
    "\n",
    "    for model_name, ModelClass in models.items():\n",
    "        print(f\"Running model: {model_name}\")\n",
    "        ap_scores = []\n",
    "        auc_scores = []\n",
    "        auc_paper_scores = []\n",
    "        auc_capymoa_scores = []\n",
    "        execution_times = []\n",
    "        model_results = []\n",
    "\n",
    "        for run in range(n_runs):\n",
    "            learner = ModelClass(schema)\n",
    "            evaluator = AnomalyDetectionEvaluator(schema)\n",
    "\n",
    "            stream.restart()  # Restart stream for each run\n",
    "            anomaly_scores = []\n",
    "            start_time = time.time()\n",
    "\n",
    "            while stream.has_more_instances():\n",
    "                instance = stream.next_instance()\n",
    "                proba = learner.score_instance(instance)\n",
    "                #We do 1-proba because for capyMOA models 1 means normal and 0 means anomaly, inverse as in streamrhf\n",
    "                anomaly_scores.append(1-proba)\n",
    "                evaluator.update(instance.y_index, proba)\n",
    "                learner.train(instance)\n",
    "\n",
    "            # Get AUC from evaluator\n",
    "            auc_score_capymoa = evaluator.auc()\n",
    "\n",
    "            #####################################\n",
    "            anomaly_scores = np.array(anomaly_scores)\n",
    "            ap_score = average_precision_score(labels, anomaly_scores)\n",
    "            auc_score = roc_auc_score(labels, anomaly_scores)\n",
    "            fpr, tpr, thresholds = roc_curve(labels, anomaly_scores)\n",
    "            auc_paper = auc(fpr, tpr)\n",
    "            #####################################\n",
    "\n",
    "            execution_time = time.time() - start_time\n",
    "\n",
    "            print(f\"Run {run + 1}: AP = {ap_score:.4f}, AUC = {auc_score:.4f}, Time = {execution_time:.2f}s\")\n",
    "\n",
    "            # Save run results\n",
    "            run_result = {\n",
    "                'Dataset': dataset_name,\n",
    "                'Model': model_name,\n",
    "                'Run': run + 1,\n",
    "                'AP': ap_score,\n",
    "                'AUC_capymoa': auc_score_capymoa,\n",
    "                'AUC (sklearn)': auc_score,\n",
    "                'AUC (paper)': auc_paper,\n",
    "                'Execution Time (s)': execution_time\n",
    "            }\n",
    "            model_results.append(run_result)\n",
    "            all_results.append(run_result)\n",
    "\n",
    "        # Save checkpoint after each model\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        results_df.to_csv(\"all_run_results_checkpoint.csv\", index=False)\n",
    "        print(f\"Checkpoint saved for model {model_name}\")\n",
    "\n",
    "        # Summarize results for the model\n",
    "        ap_scores = np.array([res['AP'] for res in model_results])\n",
    "        auc_scores = np.array([res['AUC (sklearn)'] for res in model_results])\n",
    "        auc_paper_scores = np.array([res['AUC (paper)'] for res in model_results])\n",
    "        auc_capymoa_scores = np.array([res['AUC_capymoa'] for res in model_results])\n",
    "        execution_times = np.array([res['Execution Time (s)'] for res in model_results])\n",
    "\n",
    "        mean_ap = np.mean(ap_scores)\n",
    "        mean_auc = np.mean(auc_scores)\n",
    "        mean_auc_paper = np.mean(auc_paper_scores)\n",
    "        mean_auc_capymoa = np.mean(auc_capymoa_scores)\n",
    "        mean_time = np.mean(execution_times)\n",
    "        ap_sem = sem(ap_scores)\n",
    "        auc_sem = sem(auc_scores)\n",
    "        auc_paper_sem = sem(auc_paper_scores)\n",
    "        auc_capymoa_sem = sem(auc_capymoa_scores)\n",
    "        time_sem = sem(execution_times)\n",
    "        confidence_level = 1.96\n",
    "        ap_ci = confidence_level * ap_sem\n",
    "        auc_ci = confidence_level * auc_sem\n",
    "        auc_paper_ci = confidence_level * auc_paper_sem\n",
    "        auc_capymoa_ci = confidence_level * auc_capymoa_sem\n",
    "        time_ci = confidence_level * time_sem\n",
    "\n",
    "        print(f\"Summary for {model_name}:\")\n",
    "        print(f\"AP: {mean_ap:.4f} ± {ap_ci:.4f} (95% CI)\")\n",
    "        print(f\"AUC (sklearn): {mean_auc:.4f} ± {auc_ci:.4f} (95% CI)\")\n",
    "        print(f\"AUC (paper): {mean_auc_paper:.4f} ± {auc_paper_ci:.4f} (95% CI)\")\n",
    "        print(f\"AUC (CapyMOA): {mean_auc_capymoa:.4f} ± {auc_capymoa_ci:.4f} (95% CI)\")\n",
    "        print(f\"Time: {mean_time:.2f} ± {time_ci:.2f} seconds (95% CI)\")\n",
    "\n",
    "        # Save summary\n",
    "        summary = {\n",
    "            'Dataset': dataset_name,\n",
    "            'Model': model_name,\n",
    "            'Metric': ['AP', 'AUC (sklearn)', 'AUC (paper)', 'AUC (CapyMOA)', 'Execution Time'],\n",
    "            'Mean': [mean_ap, mean_auc, mean_auc_paper, mean_auc_capymoa, mean_time],\n",
    "            'CI (95%)': [ap_ci, auc_ci, auc_paper_ci, auc_capymoa_ci, time_ci]\n",
    "        }\n",
    "        summary_df = pd.DataFrame(summary)\n",
    "\n",
    "        # Create a folder for each model in the current working directory if it doesn't exist\n",
    "        model_folder = os.path.join(os.getcwd(), model_name)\n",
    "        os.makedirs(model_folder, exist_ok=True)\n",
    "        \n",
    "        # Save the summary in the respective model's folder\n",
    "        summary_df.to_csv(os.path.join(model_folder, f\"{dataset_name}_summary.csv\"), index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results to a final CSV\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df.to_csv(\"all_run_results.csv\", index=False)\n",
    "print(\"Final results saved to 'all_run_results.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
